import streamlit as st
import cv2
import numpy as np
import pyaudio
import json
import time
import difflib
import random
import struct
import plotly.graph_objs as go
import vosk
import mediapipe as mp
from pathlib import Path
import librosa

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â–¶ ì „ì—­ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LIPS_OUTER = [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291]  # ì…ìˆ  ì™¸ê³½ë§Œ ì‚¬ìš©
UPPER_LIP = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]
LOWER_LIP = [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â–¶ ë¬¸ì¥ ë¶ˆëŸ¬ì˜¤ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_sentences_from_file(filepath="sentences.txt"):
    with open(filepath, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip()]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â–¶ ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ë¶„ì„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_rms(block):
    count = len(block) // 2
    format = f"{count}h"
    shorts = struct.unpack(format, block)
    shorts = np.array(shorts, dtype=np.int16)
    return np.sqrt(np.mean(shorts ** 2))

def estimate_pitch(audio_data, rate):
    try:
        audio_float = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32)
        pitches, magnitudes = librosa.piptrack(y=audio_float, sr=rate)
        pitch_values = pitches[magnitudes > np.median(magnitudes)]
        if len(pitch_values) > 0:
            return np.mean(pitch_values)
    except:
        pass
    return 0.0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â–¶ ì‹œê°í™” ì´ˆê¸° ê·¸ë˜í”„ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def init_plot():
    layout = go.Layout(
        title="ğŸ“Š ì‹¤ì‹œê°„ ìŒì„± ë¶„ì„",
        xaxis=dict(title="Time"),
        yaxis=dict(title="Value"),
        height=300,
    )
    fig = go.Figure(layout=layout)
    fig.add_trace(go.Scatter(y=[], mode="lines", name="Volume (dB)", line=dict(color="blue")))
    fig.add_trace(go.Scatter(y=[], mode="lines", name="Pitch (Hz)", line=dict(color="red")))
    return fig

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â–¶ ë©”ì¸ ë¶„ì„ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def recognize_audio_video_with_prompt(model_path="vosk-model-small-ko", num_sentences=5):
    st.subheader("ğŸ¤ ì£¼ì–´ì§„ ë¬¸ì¥ì„ ë°œìŒí•˜ê³  ì…ëª¨ì–‘ì„ ë”°ë¼ í•˜ì„¸ìš”!")

    model = vosk.Model(model_path)
    recognizer = vosk.KaldiRecognizer(model, 16000)
    p = pyaudio.PyAudio()
    RATE = 16000
    stream = p.open(format=pyaudio.paInt16, channels=1, rate=RATE, input=True, frames_per_buffer=4096)
    stream.start_stream()

    cap = cv2.VideoCapture(0)
    st_frame = st.empty()
    result_text = st.empty()
    audio_info = st.empty()
    graph_plot = st.empty()

    mp_face_mesh = mp.solutions.face_mesh
    all_sentences = load_sentences_from_file("sentences.txt")
    sentences = random.sample(all_sentences, num_sentences)
    scores = []

    fig = init_plot()
    db_history = []
    pitch_history = []
    x_labels = []

    with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
        idx = 0
        st.subheader(f"ğŸ‘‰ ë°œìŒí•  ë¬¸ì¥: `{sentences[idx]}`")

        while cap.isOpened() and idx < num_sentences:
            ret, frame = cap.read()
            if not ret:
                break

            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = face_mesh.process(frame_rgb)
            h, w, _ = frame.shape
            lip_open = 0

            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    upper = [(int(face_landmarks.landmark[i].x * w), int(face_landmarks.landmark[i].y * h)) for i in UPPER_LIP]
                    lower = [(int(face_landmarks.landmark[i].x * w), int(face_landmarks.landmark[i].y * h)) for i in LOWER_LIP]

                    if upper and lower:
                        upper_mean = np.mean(upper, axis=0)
                        lower_mean = np.mean(lower, axis=0)
                        lip_open = np.linalg.norm(upper_mean - lower_mean)

                    # ê¹”ë”í•œ ì…ìˆ  ìœ¤ê³½ì„  (ì™¸ê³½ë§Œ)
                    outer_points = [(int(face_landmarks.landmark[i].x * w), int(face_landmarks.landmark[i].y * h)) for i in LIPS_OUTER]
                    curve = cv2.approxPolyDP(np.array(outer_points, dtype=np.int32), epsilon=1.5, closed=True)
                    cv2.polylines(frame, [curve], isClosed=True, color=(0, 255, 0), thickness=2)

                    # ğŸ‘‰ ìœ—ì…ìˆ  ìœ¤ê³½ ì¶”ê°€
                    upper_points = [(int(face_landmarks.landmark[i].x * w), int(face_landmarks.landmark[i].y * h)) for i in UPPER_LIP]
                    upper_curve = cv2.approxPolyDP(np.array(upper_points, dtype=np.int32), epsilon=1.5, closed=False)
                    cv2.polylines(frame, [upper_curve], isClosed=False, color=(255, 0, 0), thickness=2)

            st_frame.image(frame, channels="BGR")

            data = stream.read(4096, exception_on_overflow=False)
            rms = get_rms(data)
            pitch = estimate_pitch(data, RATE)
            volume_db = 20 * np.log10(rms) if rms > 0 else -100

            audio_info.markdown(f"ğŸ”Š **ë³¼ë¥¨**: `{round(volume_db, 2)} dB` | ğŸµ **ìŒ ë†’ì´**: `{round(pitch, 2)} Hz`")

            # ì‹¤ì‹œê°„ Plotly ê·¸ë˜í”„ ê°±ì‹ 
            db_history.append(round(volume_db, 2))
            pitch_history.append(round(pitch, 2))
            x_labels.append(time.strftime("%H:%M:%S"))

            fig.data[0].y = db_history[-50:]
            fig.data[1].y = pitch_history[-50:]
            fig.update_layout(xaxis=dict(tickvals=list(range(len(x_labels[-50:]))), ticktext=x_labels[-50:]))
            graph_plot.plotly_chart(fig, use_container_width=True)

            if recognizer.AcceptWaveform(data):
                result = json.loads(recognizer.Result())
                detected_text = result["text"]

                if not detected_text.strip():
                    continue

                similarity = difflib.SequenceMatcher(None, sentences[idx].replace(" ", ""), detected_text.replace(" ", "")).ratio()
                percentage = round(similarity * 100, 2)
                mouth_score = 1.0 if lip_open >= 5 else 0.8
                final_score = percentage * mouth_score
                scores.append(final_score)

                result_text.markdown(f"""
                - ğŸ¯ ëª©í‘œ ë¬¸ì¥: `{sentences[idx]}`
                - ğŸ—£ ì¸ì‹ëœ ë¬¸ì¥: `{detected_text}`
                - ğŸ“ ë°œìŒ ì •í™•ë„: **{percentage}%**
                - ğŸ‘„ ì…ëª¨ì–‘ ì ìˆ˜: **{mouth_score * 100}%**
                - ğŸ“Š ìµœì¢… í‰ê°€ ì ìˆ˜: **{round(final_score, 2)}%**
                """)

                if final_score >= 80:
                    idx += 1
                    if idx < num_sentences:
                        st.subheader(f"ğŸ‘‰ ë‹¤ìŒ ë¬¸ì¥: `{sentences[idx]}`")
                    time.sleep(1)

            if cv2.waitKey(1) & 0xFF == 27:
                break

    cap.release()
    cv2.destroyAllWindows()
    stream.stop_stream()
    stream.close()
    p.terminate()

    avg_score = sum(scores) / len(scores) if scores else 0
    st.success(f"âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ! í‰ê·  ë°œìŒ ì •í™•ë„: **{round(avg_score, 2)}%**")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â–¶ Streamlit ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    st.title("ğŸ—£ï¸ ì‹¤ì‹œê°„ ë°œìŒ & ì…ëª¨ì–‘ í‰ê°€")
    app_mode = st.selectbox("ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”", ["Word Pronunciation Accuracy"])

    if app_mode == "Word Pronunciation Accuracy":
        recognize_audio_video_with_prompt()

if __name__ == "__main__":
    main()
